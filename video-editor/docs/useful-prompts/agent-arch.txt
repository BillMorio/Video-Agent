I seee. Now here is what the workflow will look like, I will input a script or audio. Let's assume I start with a script. This script is passed into an audio generation model in this case Eleven Labs. Then when the audio is returned, it will be transcribed with word level timestamps, these transcription will be passed to an llm that will do the reasoning and return a json that will have the different scenes, with their visual style category, whether, a-rolls, b-rolls, images, or motions graphics, then these scene json will be logged into a supabase data base.... Now, here is where the agentic workflow will start, the agent will pull each scene one by one, determine what type of scene it is if it is an a-roll scene, it will use a tool that calls heygen(an avatar generation API), the input for the heygen avatar is an audio url, an avatar id, and an optional prompt, then when the generation is done, we will log that to the appropriate scene, then move to the next scene... For scenes that are image or motion graphics scenes, we can just skip those and log them as something like(awaiting human input), for b-roll footage scene we will use a pexels api tool and generate the footage for that, and the loop continues untill the agent is done. Now, I'm not sure that logging the scenes to the database will save us memory so the agent will not have to remember everything(more guidance on that). Now, give me a very comprehensive doc that captures this... Even if it is very long. 